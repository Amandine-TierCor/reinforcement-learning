{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Reinforcement Learning \ud83d\udd17 Reinforcement Learning section of the Algorithms in Machine Learning class at ISAE-Supaero Home Github repository Syllabus \ud83d\udd17 This class covers an introduction to Reinforcement Learning (RL) in 18 hours, over 6 sessions. It aims to provide both a solid theoretical foundation and a quick learning curve towards current Deep RL algorithms. It starts with the fundamental notions underlying RL: Markov Decision Processes, model-based resolution approaches including Dynamic Programming, sample-based resolution of the Bellman equation. This leads to the identification of the three bottomline challenges in RL: function approximation, the exploration/exploitation trade-off and the search for optimality. This provides perspective to the following classes that introduce methods designed to tackle these challenges, including Deep RL methods. By the end of the class, students should be able to understand the literature on RL, implement key algorithms, and anticipate the difficulties of applying RL to various problems. Class material \ud83d\udd17 The class is split into a series of notebooks that serve as lecture material, textbook and exercice book. Open the notebooks in Binder: Additional resources \ud83d\udd17 Great books available online: Reinforcement Learning, an introduction Algorithms for Reinforcement Learning An introduction to Deep Reinforcement Learning FAQ on installing Gym for Mac users Class schedule \ud83d\udd17 Schedule MDPs and their resolution 08h30 - 11h45 03/02/2021 RL intuitions, Markov Decision Processes, Dynamic Programming Sample-based policy search 08h30 - 11h45 09/02/2021 Formulations of RL algorithms, Temporal Differences, Q-learning, the 3 bottlenecks of RL Value function approximation 13h00 - 16h15 09/02/2021 Linear approximations, Deep Q-Networks Policy gradients 09h00 - 12h15 15/02/2021 PG and Deep PG methods MCTS 09h00 - 12h15 17/02/2021 Monte Carlo Tree Search open 13h45 - 17h00 17/02/2021 open session on an RL challenge","title":"Overview"},{"location":"index.html#reinforcement-learning","text":"Reinforcement Learning section of the Algorithms in Machine Learning class at ISAE-Supaero Home Github repository","title":"Reinforcement Learning"},{"location":"index.html#syllabus","text":"This class covers an introduction to Reinforcement Learning (RL) in 18 hours, over 6 sessions. It aims to provide both a solid theoretical foundation and a quick learning curve towards current Deep RL algorithms. It starts with the fundamental notions underlying RL: Markov Decision Processes, model-based resolution approaches including Dynamic Programming, sample-based resolution of the Bellman equation. This leads to the identification of the three bottomline challenges in RL: function approximation, the exploration/exploitation trade-off and the search for optimality. This provides perspective to the following classes that introduce methods designed to tackle these challenges, including Deep RL methods. By the end of the class, students should be able to understand the literature on RL, implement key algorithms, and anticipate the difficulties of applying RL to various problems.","title":"Syllabus"},{"location":"index.html#class-material","text":"The class is split into a series of notebooks that serve as lecture material, textbook and exercice book. Open the notebooks in Binder:","title":"Class material"},{"location":"index.html#additional-resources","text":"Great books available online: Reinforcement Learning, an introduction Algorithms for Reinforcement Learning An introduction to Deep Reinforcement Learning FAQ on installing Gym for Mac users","title":"Additional resources"},{"location":"index.html#class-schedule","text":"Schedule MDPs and their resolution 08h30 - 11h45 03/02/2021 RL intuitions, Markov Decision Processes, Dynamic Programming Sample-based policy search 08h30 - 11h45 09/02/2021 Formulations of RL algorithms, Temporal Differences, Q-learning, the 3 bottlenecks of RL Value function approximation 13h00 - 16h15 09/02/2021 Linear approximations, Deep Q-Networks Policy gradients 09h00 - 12h15 15/02/2021 PG and Deep PG methods MCTS 09h00 - 12h15 17/02/2021 Monte Carlo Tree Search open 13h45 - 17h00 17/02/2021 open session on an RL challenge","title":"Class schedule"},{"location":"RL0-intro.html","text":"Introduction to Reinforcement Learning \ud83d\udd17 Home Github repository This class offers a \"getting started\" session on Reinforcement Learning. It lays the main intuitions that will be formalized in subsequent lectures, provides additional resources and software requirements. Notebook Additional resources \ud83d\udd17 Great books available online: Reinforcement Learning, an introduction Algorithms for Reinforcement Learning An introduction to Deep Reinforcement Learning FAQ on installing Gym for Mac users (many thanks to the 2021 students) The AlphaGo movie","title":"Introduction to RL"},{"location":"RL0-intro.html#introduction-to-reinforcement-learning","text":"Home Github repository This class offers a \"getting started\" session on Reinforcement Learning. It lays the main intuitions that will be formalized in subsequent lectures, provides additional resources and software requirements. Notebook","title":"Introduction to Reinforcement Learning"},{"location":"RL0-intro.html#additional-resources","text":"Great books available online: Reinforcement Learning, an introduction Algorithms for Reinforcement Learning An introduction to Deep Reinforcement Learning FAQ on installing Gym for Mac users (many thanks to the 2021 students) The AlphaGo movie","title":"Additional resources"},{"location":"RL1-MDP.html","text":"Markov Decision Processes \ud83d\udd17 Home Github repository The previous class provided the key intuitions about RL. RL is about learning to control dynamic systems. This class provides an introduction to the model underlying all Reinforcement Learning theory and developments: Markov Decision Processes. Notebook","title":"Markov Decision Processes"},{"location":"RL1-MDP.html#markov-decision-processes","text":"Home Github repository The previous class provided the key intuitions about RL. RL is about learning to control dynamic systems. This class provides an introduction to the model underlying all Reinforcement Learning theory and developments: Markov Decision Processes. Notebook","title":"Markov Decision Processes"},{"location":"RL2-DP.html","text":"Bellman equations, characterizing optimal policies \ud83d\udd17 Home Github repository The previous class (RL1) introduced the model of Markov Decision Processes as a way to describe discrete-time, stochastic, dynamical systems. Our focus is on controling such systems. For this we want to characterize what makes a policy optimal and how to find it. This class covers the model-based resolution of MDPs, in particular via Dynamic Programming. During class we will cover sections 1 to 5 of the notebook. Sections 6 and 7 are extra content that is important for a better understanding of the concepts at stake but will not be covered in class and will not be directly reused in future classes. Notebook","title":"Solving MDPs"},{"location":"RL2-DP.html#bellman-equations-characterizing-optimal-policies","text":"Home Github repository The previous class (RL1) introduced the model of Markov Decision Processes as a way to describe discrete-time, stochastic, dynamical systems. Our focus is on controling such systems. For this we want to characterize what makes a policy optimal and how to find it. This class covers the model-based resolution of MDPs, in particular via Dynamic Programming. During class we will cover sections 1 to 5 of the notebook. Sections 6 and 7 are extra content that is important for a better understanding of the concepts at stake but will not be covered in class and will not be directly reused in future classes. Notebook","title":"Bellman equations, characterizing optimal policies"},{"location":"RL3-eval.html","text":"Evaluating policies with samples \ud83d\udd17 Home Github repository The previous classes introduced MDPs and the Bellman equations (evaluation and optimality). These equations involve the MDP's model (transition and reward functions). We saw how to solve these equations using the model. In this class, we will investigate how one can aim to solve the evaluation equation with samples rather than with the model. During class we will cover sections 1 to 6 of the notebook. Sections 7 to 10 are extra content that is important for a better understanding of the concepts at stake but will not be covered in class and will not be directly reused in future classes. Notebook","title":"Evaluating policies"},{"location":"RL3-eval.html#evaluating-policies-with-samples","text":"Home Github repository The previous classes introduced MDPs and the Bellman equations (evaluation and optimality). These equations involve the MDP's model (transition and reward functions). We saw how to solve these equations using the model. In this class, we will investigate how one can aim to solve the evaluation equation with samples rather than with the model. During class we will cover sections 1 to 6 of the notebook. Sections 7 to 10 are extra content that is important for a better understanding of the concepts at stake but will not be covered in class and will not be directly reused in future classes. Notebook","title":"Evaluating policies with samples"},{"location":"RL4-control.html","text":"Solving the optimality equation with samples \ud83d\udd17 Home Github repository The last class showed we can learn a policy's value function using only interaction samples. In this class, we focus on solving the optimality equation and estimating optimal value functions and policies from interaction samples. We will cover temporal difference based algorithms such as Q-learning and SARSA. Notebook","title":"Model-free control"},{"location":"RL4-control.html#solving-the-optimality-equation-with-samples","text":"Home Github repository The last class showed we can learn a policy's value function using only interaction samples. In this class, we focus on solving the optimality equation and estimating optimal value functions and policies from interaction samples. We will cover temporal difference based algorithms such as Q-learning and SARSA. Notebook","title":"Solving the optimality equation with samples"},{"location":"RL5-DQN.html","text":"Deep Q-Networks \ud83d\udd17 Home Github repository In the previous classes we saw that one could replace the model-based value iteration process by an approximate value iteration one. When the approximation is done by performing stochastic approximation, we obtain the $Q$ learning algorithm. We saw it was straightforward to extend this to use experience replay memories and batch stochastic gradient descent. In this class, we combine the stochastic gradient descent approach with replay memories and represent $Q$ as a neural network. This yields the Deep Q-Networks algorithm. Notebook","title":"Deep Q-Networks"},{"location":"RL5-DQN.html#deep-q-networks","text":"Home Github repository In the previous classes we saw that one could replace the model-based value iteration process by an approximate value iteration one. When the approximation is done by performing stochastic approximation, we obtain the $Q$ learning algorithm. We saw it was straightforward to extend this to use experience replay memories and batch stochastic gradient descent. In this class, we combine the stochastic gradient descent approach with replay memories and represent $Q$ as a neural network. This yields the Deep Q-Networks algorithm. Notebook","title":"Deep Q-Networks"}]}