{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 4: Temporal difference policy optimization.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous classes, we saw how value functions can be learned from samples. We now turn to the problem of learning the optimal value function for a given MDP. Model-based policy optimization provided us with the value and policy iteration methods. We investigate how we can learn their solutions via sampling and stochastic approximation to replace the knowledge of a model.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Prerequisites:**\n",
    "- Stochastic Approximation / Stochastic Gradient Descent\n",
    "- MDPs, policies, value functions, stationary distribution\n",
    "- Value Iteration, Policy Iteration\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything you need to know"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder\n",
    "\n",
    "- Value Iteration\n",
    "- Policy Iteration\n",
    "- TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Policy Iteration as Stochastic Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Policy Iteration based on TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous classes, we have seen that we could learn a policy's $Q$ function with temporal differences, yielding the TD(0) method.\n",
    "\n",
    "But we also have seen that given a policy's (approximate) $Q$ function, we can define the greedy policy $\\mathcal{G}(Q)$ with respect to $Q$. Repeating this evaluation and improvement process provided the (approximate) policy iteration algorithm that lead to (a neighborhood of) an optimal policy.\n",
    "\n",
    "So let's start with this.  \n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice**  \n",
    "Write a model-free approximate policy iteration algorithm that uses TD(0) on $Q$ functions with random behavior policy, as a method to obtain a policy $\\pi_n$'s approximate value function $Q_n$. Take $\\pi_{n+1} = \\mathcal{G}(Q)$ every $N=1000$ steps of TD(0).  \n",
    "Test this algorithm on FrozenLake with $\\gamma=0.9$, $\\alpha=0.001$ and for $2000000$ time steps.  \n",
    "Use a model-based policy evaluation function and a value iteration algorithm from previous classes to compute $Q^*$ and monitor the difference between $Q^\\pi$ and $Q^*$ after each policy update.  \n",
    "As an initial guess, initialize $Q$ to zero and take the greedy policy.  \n",
    "Don't hesitate to (massively) re-use solutions from past classes' exercices.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/RL4_exercice1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=np.int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def print_policy(pi):\n",
    "    actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "def value_iteration(V,epsilon,max_iter):\n",
    "    W = np.copy(V)\n",
    "    residuals = np.zeros((max_iter))\n",
    "    for i in range(max_iter):\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q = np.zeros((env.action_space.n))\n",
    "            for a in range(env.action_space.n):\n",
    "                outcomes = env.unwrapped.P[s][a]\n",
    "                for o in outcomes:\n",
    "                    p  = o[0]\n",
    "                    s2 = o[1]\n",
    "                    r  = o[2]\n",
    "                    Q[a] += p*(r+gamma*V[s2])\n",
    "            W[s] = np.max(Q)\n",
    "            #print(W[s])\n",
    "        residuals[i] = np.max(np.abs(W-V))\n",
    "        #print(\"abs\", np.abs(W-V))\n",
    "        np.copyto(V,W)\n",
    "        if residuals[i]<epsilon:\n",
    "            residuals = residuals[:i+1]\n",
    "            break\n",
    "    return V, residuals\n",
    "\n",
    "def Q_from_V(V):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[s][a]\n",
    "            for o in outcomes:\n",
    "                p  = o[0]\n",
    "                s2 = o[1]\n",
    "                r  = o[2]\n",
    "                Q[s,a] += p*(r+gamma*V[s2])\n",
    "    return Q\n",
    "\n",
    "def policy_eval_iter_mat(pi, max_iter):\n",
    "    # build r and P\n",
    "    r_pi = np.zeros((env.observation_space.n))\n",
    "    P_pi = np.zeros((env.observation_space.n, env.observation_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        outcomes = env.unwrapped.P[x][pi[x]]\n",
    "        for o in outcomes:\n",
    "            p = o[0]\n",
    "            y = o[1]\n",
    "            r = o[2]\n",
    "            P_pi[x,y] += p\n",
    "            r_pi[x] += r*p\n",
    "    # Compute V\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    for i in range(max_iter):\n",
    "        V = r_pi + gamma * np.dot(P_pi, V)\n",
    "    return V\n",
    "\n",
    "# Policy definition and parameters\n",
    "Q0  = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "max_steps=2000000\n",
    "update_period=500000\n",
    "optimality_gap = []\n",
    "\n",
    "# Model-based optimisation\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(Vstar)\n",
    "\n",
    "Q = np.copy(Q0)\n",
    "x = env.reset()\n",
    "for t in range(max_steps):\n",
    "    # update policy every N steps\n",
    "    if t%update_period==0:\n",
    "        pi = greedyQpolicy(Q)\n",
    "        # evaluate policy (just for monitoring)\n",
    "        Qpi = Q_from_V(policy_eval_iter_mat(pi, 1000))\n",
    "        optimality_gap.append(np.max(np.abs(Qpi-Qstar)))\n",
    "    # random behavior policy\n",
    "    a = np.random.randint(4)\n",
    "    y,r,d,_ = env.step(a)\n",
    "    # TD(0) update\n",
    "    Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][pi[y]]-Q[x][a])\n",
    "    if d==True:\n",
    "        x = env.reset()\n",
    "    else:\n",
    "        x=y\n",
    "\n",
    "pi = greedyQpolicy(Q)\n",
    "Qpi = Q_from_V(policy_eval_iter_mat(pi, 1000))\n",
    "optimality_gap.append(np.max(np.abs(Qpi-Qstar)))\n",
    "print_policy(pi)\n",
    "print(pi-greedyQpolicy(Qstar)) # remember there may be several optimal policies\n",
    "plt.plot(optimality_gap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(optimality_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#PI-TD0-1000\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"PI-TD0-1000\" class=\"collapse\">\n",
    "    \n",
    "This is the type of graph you should obtain:\n",
    "<img src=\"img/PI-TD0-1000.png\" style=\"height: 200px;\"></img>\n",
    "\n",
    "As is often the case with Policy Iteration, the policy's value quickly reaches a good neighborhood of $Q^*$. But then it tends to oscillate due to the policy approximation.\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interleaving policy improvement and evaluation\n",
    "\n",
    "Now let's bring the update frequency of the policy down to 1.  \n",
    "At any given time, the algorithm has a current value function $Q$ and policy $\\pi$. Taking $N=1$ amounts to approximating $Q^{\\pi}$ by taking a single stochastic approximation step from $Q$ towards $T^{\\pi} Q$. So, after collecting sample $(s,a,r,s')$:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s',\\pi(s'))-  Q(s,a) ).$$\n",
    "\n",
    "Then, $\\pi$ is immediately redefined as $\\mathcal{G}(Q)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercice:**  \n",
    "Modify your code above to experiment with $N=1$.  \n",
    "To keep reasonable running times, only evaluate the policy every $1000$ updates.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL4_exercice2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n",
    "\n",
    "import gym\n",
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=np.int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "def to_s(row,col):\n",
    "    return row*env.unwrapped.ncol+col\n",
    "\n",
    "def print_policy(pi):\n",
    "    actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            print(actions[pi[to_s(row,col)]], end='')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "def value_iteration(V,epsilon,max_iter):\n",
    "    W = np.copy(V)\n",
    "    residuals = np.zeros((max_iter))\n",
    "    for i in range(max_iter):\n",
    "        for s in range(env.observation_space.n):\n",
    "            Q = np.zeros((env.action_space.n))\n",
    "            for a in range(env.action_space.n):\n",
    "                outcomes = env.unwrapped.P[s][a]\n",
    "                for o in outcomes:\n",
    "                    p  = o[0]\n",
    "                    s2 = o[1]\n",
    "                    r  = o[2]\n",
    "                    Q[a] += p*(r+gamma*V[s2])\n",
    "            W[s] = np.max(Q)\n",
    "            #print(W[s])\n",
    "        residuals[i] = np.max(np.abs(W-V))\n",
    "        #print(\"abs\", np.abs(W-V))\n",
    "        np.copyto(V,W)\n",
    "        if residuals[i]<epsilon:\n",
    "            residuals = residuals[:i+1]\n",
    "            break\n",
    "    return V, residuals\n",
    "\n",
    "def Q_from_V(V):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[s][a]\n",
    "            for o in outcomes:\n",
    "                p  = o[0]\n",
    "                s2 = o[1]\n",
    "                r  = o[2]\n",
    "                Q[s,a] += p*(r+gamma*V[s2])\n",
    "    return Q\n",
    "\n",
    "def policy_eval_iter_mat(pi, max_iter):\n",
    "    # build r and P\n",
    "    r_pi = np.zeros((env.observation_space.n))\n",
    "    P_pi = np.zeros((env.observation_space.n, env.observation_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        outcomes = env.unwrapped.P[x][pi[x]]\n",
    "        for o in outcomes:\n",
    "            p = o[0]\n",
    "            y = o[1]\n",
    "            r = o[2]\n",
    "            P_pi[x,y] += p\n",
    "            r_pi[x] += r*p\n",
    "    # Compute V\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    for i in range(max_iter):\n",
    "        V = r_pi + gamma * np.dot(P_pi, V)\n",
    "    return V\n",
    "\n",
    "# Policy definition and parameters\n",
    "Q0  = np.zeros((env.observation_space.n,env.action_space.n))\n",
    "gamma = 0.9\n",
    "alpha = 0.001\n",
    "max_steps=1000000\n",
    "update_period=1\n",
    "eval_period=1000\n",
    "optimality_gap = []\n",
    "\n",
    "# Model-based optimisation\n",
    "Vinit = np.zeros((env.observation_space.n))\n",
    "Vstar,residuals = value_iteration(Vinit,1e-4,1000)\n",
    "Qstar = Q_from_V(Vstar)\n",
    "\n",
    "Q = np.copy(Q0)\n",
    "x = env.reset()\n",
    "for t in range(max_steps):\n",
    "    # update policy every N steps\n",
    "    if t%update_period==0:\n",
    "        pi = greedyQpolicy(Q)\n",
    "    # evaluate policy (just for monitoring)\n",
    "    if t%eval_period==0:\n",
    "        Qpi = Q_from_V(policy_eval_iter_mat(pi, 1000))\n",
    "        optimality_gap.append(np.max(np.abs(Qpi-Qstar)))\n",
    "    # random behavior policy\n",
    "    a = np.random.randint(4)\n",
    "    y,r,d,_ = env.step(a)\n",
    "    # TD(0) update\n",
    "    Q[x][a] = Q[x][a] + alpha * (r+gamma*Q[y][pi[y]]-Q[x][a])\n",
    "    if d==True:\n",
    "        x = env.reset()\n",
    "    else:\n",
    "        x=y\n",
    "\n",
    "pi = greedyQpolicy(Q)\n",
    "Qpi = Q_from_V(policy_eval_iter_mat(pi, 1000))\n",
    "optimality_gap.append(np.max(np.abs(Qpi-Qstar)))\n",
    "print_policy(pi)\n",
    "print(pi-greedyQpolicy(Qstar)) # remember there may be several optimal policies\n",
    "plt.plot(optimality_gap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#PI-TD0-1\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"PI-TD0-1\" class=\"collapse\">\n",
    "    \n",
    "This is the type of graph you should obtain:\n",
    "<img src=\"img/PI-TD0-1.png\" style=\"height: 200px;\"></img>\n",
    "\n",
    "The method still provides a policy that is close to optimal.\n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the exercises above, one can see that a reasonably good policy can be found early, but that sometimes the policy oscillates around $\\pi^*$ with sub-optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one has a replay memory of samples, it is possible to take a minibatch of these samples and:\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s',\\pi(s'))-  Q(s,a) ).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Value Iteration as Stochastic Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section built directly from the available TD(0) algorithm to build the policy iteration sequence of value functions and policies. But we know there is an another way to obtain $Q^*$, through value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The three key challenges of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the exploration/exploitation trade-off\n",
    "- value function approximation\n",
    "- solving the maximization problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
