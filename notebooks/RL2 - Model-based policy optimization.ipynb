{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class 2: Model-based policy optimization**\n",
    "\n",
    "1. [Everything you need to know](#everything)\n",
    "2. [Reminder](#reminder)\n",
    "3. [Value Iteration](#vi)\n",
    "4. [Policy Iteration](#pi)\n",
    "5. [Policy optimization with linear programming](#lp)\n",
    "6. [Asynchronous Dynamic Programming](#adp)\n",
    "    1. [Asynchronous Value Iteration](#avi)\n",
    "    2. [Asynchronous Policy Iteration](#api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous class introduced the MDP model which is the foundation upon which RL theory is built, and characterized optimal control policies. In this class, we investigate how one can use the model algorithmically to find optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"everything\"></a>Everything you need to know\n",
    "\n",
    "\n",
    "Everything you should remember after this session.\n",
    "<div class=\"alert alert-success\">\n",
    "<ul>\n",
    "<li> Value Iteration: finding $V^*$ or $Q^*$ by repeatedly applying $T^*$ to any initial function.\n",
    "<li> Policy Iteration: finding $V^*$ or $Q^*$ and $\\pi^*$ by building the sequence $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ that converges to $\\pi^*$. Repeatedly alternates an evaluation and an improvement phase.\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"reminder\"></a>Reminder\n",
    "\n",
    "Recall the main results from the previous class.\n",
    "\n",
    "We have introduced the general **discrete-time stochastic optimal control problem**:\n",
    "- Environment (discrete time, non-deterministic, non-linear, Markov) $\\leftrightarrow$ MDP.\n",
    "- Behaviour $\\leftrightarrow$ control policy $\\pi : s\\mapsto a$.\n",
    "- Policy evaluation criterion $\\leftrightarrow$ $\\gamma$-discounted criterion.\n",
    "- Goal $\\leftrightarrow$ Maximize value function $V^\\pi(s)$, $Q^\\pi(s,a)$.\n",
    "- Evaluation eq. $\\leftrightarrow$ $V^\\pi = T^\\pi V^\\pi$, $Q^\\pi = T^\\pi Q^\\pi$, linear equation, contraction mapping.\n",
    "- Bellman optimality eq. $\\leftrightarrow$ $V^* = T^* V^*$, $Q^* = T^* Q^*$, non-linear equation, contraction mapping.\n",
    "\n",
    "Based on the analysis of the previous section, we introduce several algorithms that will lay a solid ground for model-free RL algorithms in the next class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"vi\"></a>Value Iteration\n",
    "\n",
    "Value iteration is actually the algorithm we defined in the `vf_optim` function in the correction of the previous class' last exercice. It directly exploits the contraction mapping property of $T^*$ and iterates over value functions in order to converge to $V^*$. Once, $V^*$ is found, finding the optimal policy is a matter of writing $Q^*$ and defining a policy that is $Q^*$-greedy.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Write the pseudo-code of Value Iteration.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersVI\" class=\"collapse\">\n",
    "    Input data: $V$, $\\epsilon$<br>\n",
    "    Init: $\\Delta = \\epsilon+1$<br>\n",
    "    While $\\Delta \\geq \\epsilon$:<br>\n",
    "    &nbsp;&nbsp;&nbsp; For $s \\in S$:  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a\\in A$:  <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{new}(s) = \\max_a Q(s,a)$  <br>\n",
    "    &nbsp;&nbsp;&nbsp; $\\Delta = \\| V_{new}-V \\|_\\infty$  <br>\n",
    "    &nbsp;&nbsp;&nbsp; $V = V_{new}$  <br>\n",
    "    Return $V$  <br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Compute and display an optimal policy for the FrozenLake game, using Value Iteration. If you copy-paste the results of exercices 2, 3 and 8 of the previous class, you almost don't need to write any new code.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercice1.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those familiar with the principles of Dynamic Programming will note that Value Iteration is a Dynamic Programming algorithm that operates in value function space, monotonically hopping from value function to value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "What is the time complexity of Value Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersComplexVI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersComplexVI\" class=\"collapse\">\n",
    "$O(S^2 A)$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"pi\"></a>Policy Iteration\n",
    "\n",
    "The Policy Iteration algorithm stems from the following remark. Suppose we have a policy $\\pi$ and know its value function $V^\\pi$ and state-action value function $Q^\\pi$. Then, the non-stationary policy $\\pi'$ that acts greedily with respect to $Q^\\pi$ for the first time step and then follows $\\pi$ has a value function $V^{\\pi'}$ that is greater or equal to $V^\\pi$ (equal if $\\pi$ is optimal, strictly greater otherwise). Actually, the contraction property of $T^*$ insures that the stationary policy $\\pi'$ that is greedy with respect to $Q^\\pi$ is at least as good as $\\pi$, that is $V^{\\pi'}\\geq V^\\pi$. Consequently, the sequence of policies defined by $\\pi_{n+1}(s) = \\arg\\max_{a\\in A} Q^{\\pi_n}(s,a)$ has a monotonically improving corresponding sequence of value functions $V^{\\pi_n}$ and converges to $\\pi^*$.<br>\n",
    "<br>\n",
    "Let's make this more simple with a drawing. Policy iteration alternates two phases:\n",
    "1. Evaluate $\\pi_n$ $\\rightarrow Q^{\\pi_n}$\n",
    "2. Compute $\\pi_{n+1}$ as the $Q^{\\pi_n}$-greedy policy\n",
    "\n",
    "<img src=\"img/policyiteration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process above defines a sequence of policies **and** value functions. Since, for finite state and action spaces, the number of policies is finite, Policy Iteration is guaranteed to converge in a finite number of iterations.\n",
    "\n",
    "However, $\\pi_{n+1} = \\pi_n$ is not a valid termination criterion for Policy Iteration!\n",
    "\n",
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Can you explain why? What would be a sound termination criterion?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersPIstop\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersPIstop\" class=\"collapse\">\n",
    "    \n",
    "The improvement step of policy iteration consists in computing $\\pi_{n+1}(s) = \\arg\\max_a Q_n(s,a)$.  \n",
    "But there is no reason for the maximizing action to be unique.  \n",
    "In the case of multiple maximizing actions, the sequence of policies $\\pi_n$ could very well oscillate between several equivalent optimal policies.  \n",
    "This important lesson to keep from this is that the Bellman equation guarantees that $V^*$ is unique, but also that several policies might be optimal (and have value $V^*$).  \n",
    "    \n",
    "Also, oscillation between two quasi-equivalent policies might occur when the computation of $Q^{\\pi_n}$ carries small errors.  \n",
    "    \n",
    "\n",
    "One solution is to define a tie-breaker between equivalent actions. However, even if its a bit more costly computationally, it might be safer to compare $V^{\\pi_{n+1}}$ and $V^{\\pi_n}$ and allow a precision error of $\\epsilon$. To observe this phenomenon, one could compare two versions of Policy Iteration: one that uses the `policy_eval_iter_mat` and one that uses the `policy_eval_lin` functions from exercice 6 of the previous class. The latter carries the errors of matrix inversion which causes the algorithm to never terminate and alternate between equivalent optimal policies.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Write the pseudo-code of Policy Iteration using the contraction property of $T^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersPI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersPI\" class=\"collapse\">\n",
    "    Input data: $\\pi$, $\\epsilon$<br>\n",
    "    Init: $\\Delta = \\Delta_\\pi = \\epsilon+1$<br>\n",
    "    While $\\Delta_\\pi \\geq \\epsilon$:<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy evaluation, solve $V=T^\\pi V$<br>\n",
    "&nbsp;&nbsp;&nbsp; $V_{old} = V$<br>\n",
    "&nbsp;&nbsp;&nbsp; While $\\Delta \\geq \\epsilon$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $s \\in S$:<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V_{new}(s) = r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\Delta = \\| V_{new}-V \\|_\\infty$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $V = V_{new}$<br>\n",
    "&nbsp;&nbsp;&nbsp; $\\Delta_\\pi = \\|V - V_{old} \\|_\\infty$<br>\n",
    "&nbsp;&nbsp;&nbsp; # Policy improvement<br>\n",
    "&nbsp;&nbsp;&nbsp; For $s \\in S$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; For $a \\in A$: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $Q(s,a) = r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\pi(s) = \\max_a Q(s,a)$<br>\n",
    "    Return $\\pi$  <br>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Compute and display an optimal policy for the FrozenLake game, using Policy Iteration. For the sake of simplicity, use a fixed number of iterations for the resolution of $V=T^\\pi V$. Recall that exercice 6 of the previous class provided you with a function that computes the value $V^\\pi$ of any policy, that exercice 2 transformed a $V$ function into a $Q$ function, and that exercice 3 picked the greedy policy from a $Q$ function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/RL2_exercice2.py\n",
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line above to load a correction in this cell (then you can execute this code).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "What is the time complexity of Policy Iteration in terms of $|S|$ and $|A|$?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answersComplexPI\" data-toggle=\"collapse\"><b>Answer:</b></a><br>\n",
    "<div id=\"answersComplexPI\" class=\"collapse\">\n",
    "$O(|S|^2 |A|)$\n",
    "    \n",
    "Policy Iteration has the same time complexity as Value Iteration. In practice, for finite state and action spaces, Policy Iteration converges in a finite number of steps (contrarily to Value Iteration). But each of these steps requires the resolution of $V = T^\\pi V$ which is where the real computational cost is.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for Value Iteration, those familiar with Dynamic Programming will remark that Policy Iteration is a Dynamic Programming algorithm in policy space, monotonically hopping from policy to policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"lp\"></a>Linear Programming\n",
    "\n",
    "If this is the first time you read this notebook, this part can be skipped. [Link to next part](#adp)\n",
    "\n",
    "An alternative way of finding $V^*$ is by casting the optimality equation as a linear optimization problem. This formulation is mainly given for your curiosity and we will not study it any further.<br>\n",
    "<br>\n",
    "Recall the optimality equation:\n",
    "$$\\forall s\\in S, V(s)=\\max\\limits_{a\\in A} \\left\\{r(s,a) + \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a) V(s')\\right\\}$$\n",
    "The key remark to transform this into a linear program is to rephrase it as \"$V^*$ is the smallest value that dominates over all policy values\". This can be written as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall \\pi, \\ V \\geq T^\\pi V\n",
    "\\end{array} \\right.$$\n",
    "\"For all $\\pi$\" means for all possible association $s\\leftrightarrow a$, so this can be expanded as:\n",
    "$$\\left\\{ \\begin{array}{c}\n",
    "\\min \\sum\\limits_{s\\in S} V(s)\\\\\n",
    "s.t. \\ \\forall (s,a)\\in S\\times A, \\quad V(s) - \\gamma \\sum\\limits_{s'\\in S} p(s'|s,a)V(s') \\geq r(s,a)\n",
    "\\end{array}\\right.$$\n",
    "Which, finally, is a linear program with $|S|$ variables and $|S||A|$ constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"adp\"></a>Asynchronous Dynamic Programming\n",
    "\n",
    "If this is the first time you read this notebook, this part can be skipped. [Link to next part](#rl)\n",
    "\n",
    "We have seen that Value Iteration and Policy Iteration are Dynamic Programming algorithms. They follow a path, respectively in value function and in policy space that leads to $V^*$ and $\\pi^*$. But we can remark that they both perform *state-wise* operations such as:\n",
    "\n",
    "- $V(s) \\leftarrow \\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$ for value iteration\n",
    "- $\\pi(s) \\leftarrow \\arg\\max_a Q^{\\pi}(s,a)$ for policy iteration\n",
    "- $V(s) \\leftarrow r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$ for policy evaluation\n",
    "\n",
    "These state-wise operations are called Bellman backups. Let's rename them, respectively:\n",
    "- `BBVopt(V,s): return` $\\max_{a} r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s')$\n",
    "- `BBpiopt(V,s): return` $\\arg\\max_a r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s)$\n",
    "- `BBVval(V,s): return` $r(s,\\pi(s)) + \\gamma \\sum_{s'} p(s'|s,\\pi(s)) V(s')$\n",
    "\n",
    "Then we can rewrite value iteration as:<br>\n",
    "`\n",
    "V(s) = Vinit(s) for all s\n",
    "while error>epsilon\n",
    "  for s in S\n",
    "    W(s) = BBVopt(V,s)\n",
    "  error = norm(W-V)\n",
    "  V = W\n",
    "`\n",
    "\n",
    "And policy iteration as:<br>\n",
    "`\n",
    "while(true)\n",
    "  V(s) = 0 for all s\n",
    "  for k=1 to K\n",
    "    for s in S\n",
    "      V(s) = BBVval(V,s)\n",
    "  for s in S\n",
    "    pi2 BBpiopt(V,s)\n",
    "  if(pi == pi2)\n",
    "    stop\n",
    "  else\n",
    "    pi = pi2\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"avi\"></a>Asynchronous Value Iteration\n",
    "\n",
    "Let's take the pseudo-code of value iteration. Why don't we perform directly `V(s) = BBVopt(V,s)`, instead of relying on the intermediate `W` function? Doing so is actually called **Gauss-Seidl Value Iteration** and it opens the door to a much wider class of algorithms called Asynchronous Value Iteration.<br>\n",
    "<br>\n",
    "It is crucial to note that in Gauss-Seidl Value Iteration, the order in which the states are considered for backups greatly affects of rewards are propagated through the state space and how the sequence of value functions converges to $V^*$. But still, in Gauss-Seidl Value Iteration, states are updated once per sweep over the state space.\n",
    "\n",
    "Why wouldn't we update the value of some states more often than others? Would the overall value function still converge to $V^*$? A very powerful theorem actually states that:\n",
    "<div class=\"alert alert-success\">As long as every state is visited infinitely often by the `V(s)` $\\leftarrow$ `BBVopt(V,s)` operation as time tends to $+\\infty$, the value function $V$ converges to $V^*$\n",
    "</div>\n",
    "\n",
    "Consequently, we could pick states totally randomly in order to perform Bellman backups on $V$ and $V$ would still converge to $V^*$. Although picking states randomly for that purpose seems like a bad idea, identifying a good ordering for the backups can lead to drastic improvements in convergence speed. This is the key idea of **Asynchronous Value Iteration** and has justified (among other things) the popular **Prioritized Sweeping** and **Real-Time Dynamic Programming** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"api\"></a>Asynchronous Policy Iteration\n",
    "\n",
    "Let's now take the pseudo-code of policy iteration. The evaluation step and the improvement step are clearly separated. But one can note that if we require the evaluation step to have infinite precision, `K` needs to tend to $\\infty$. So, realistic implementations of Policy Iteration, as the one we crafted earlier, tolerate some error on $V^\\pi$. But could we take an arbitrary value for `K`? Would this still converge? The answer is yes and this algorithm is known as **Modified Policy Iteration**.\n",
    "\n",
    "But then, can we push it a little further and say that `K=1`? Then in this case, Modified Policy Iteration becomes... Value Iteration (check it by yourself)!\n",
    "\n",
    "As in the value iteration case, can we update the value or policy of a given state in any ordering? The answer is yes again and the theorem states that:\n",
    "<div class=\"alert alert-success\"> As long as every state is visited infinitely often by the `V(s)` $\\leftarrow$ `BBVval(V,s)`  and the $\\pi(s) \\leftarrow$ `BBpiopt(V,s)` operations as time tends to $+\\infty$, the value function $V$ and the policy $\\pi$ converge respectively to $V^*$ and $\\pi^*$\n",
    "</div>\n",
    "\n",
    "That is the most general framework one can give for **Asynchronous Dynamic Programming** in MDP resolution. It is often called **Asynchronous Policy Iteration**.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercice:</b><br>\n",
    "Instead of reasoning on state value functions $V$, we can work directly with state-action value functions $Q$ and policies $\\pi$. Suppose we maintain a memory of a function $Q$ and a policy $\\pi$, then we can define two Bellman backups:\n",
    "<ul>\n",
    "<li> `BBQ(s,a):` $Q(s,a) \\leftarrow r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) Q(s',\\pi(s'))$\n",
    "<li> `BBpi(s):` $\\pi(s) \\leftarrow \\arg\\max_a Q(s,a)$\n",
    "</ul>\n",
    "We can note the similarity between `BBQ` and the previous `BBVval` and the one between `BBpi` and the previous `BBpiopt`.  The previous `BBVopt` is then the successive application of `BBQ` and `BBpi`.<br>\n",
    "Rewrite Value Iteration and Policy Iteration with these two elementary operations. Rephrase the key property of Asynchronous Policy Iteration using these operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\"><a href=\"#answers3\" data-toggle=\"collapse\"><b>Answers:</b></a><br>\n",
    "<div id=\"answers3\" class=\"collapse\">\n",
    "Value Iteration:\n",
    "<code>\n",
    "Q(s,a) = Qinit(s,a) for all s,a\n",
    "while error>epsilon\n",
    "  for s,a in SxA\n",
    "    BBQ(s,a)\n",
    "    BBpi(s)</code>\n",
    "<br><br>\n",
    "Policy Iteration:\n",
    "<code>\n",
    "while(pi not constant)\n",
    "  Q(s,a) = 0 for all s,a\n",
    "  while error>epsilon\n",
    "    for s,a in SxA\n",
    "      BBQ(s,a)\n",
    "  for s in S\n",
    "    BBpi(s)</code>\n",
    "<br><br>\n",
    "Asynchronous Policy Iteration: as long as all $s$ and all $a$ are visited infinitely often for Bellman backups `BBQ` and `BBpi` as time tends to $+\\infty$, $Q$ and $\\pi$ will tend respectively to $Q^*$ and $\\pi^*$ (whatever the ordering of backups).\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
